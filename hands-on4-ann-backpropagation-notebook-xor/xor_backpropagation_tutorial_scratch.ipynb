{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe962b5",
   "metadata": {},
   "source": [
    "# Tutorial: Red Neuronal Multicapa (Backpropagation) — implementación from‑scratch para XOR\n",
    "\n",
    "Notebook didáctico que cubre fundamentos teóricos, matemática del forward/backward y una implementación completa *from‑scratch* en Python (numpy) para aprender la compuerta XOR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0bab1",
   "metadata": {},
   "source": [
    "## 1. Fundamentos teóricos\n",
    "\n",
    "- Una Red Neuronal Multicapa (MLP) consta de capas: entrada → capa oculta(s) → salida.\n",
    "- Para problemas no linealmente separables (como XOR) se necesita al menos una capa oculta con activación no lineal.\n",
    "- Backpropagation: calcular gradientes usando la regla de la cadena y actualizar pesos con un optimizador (p. ej. gradiente descendente)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48e15e",
   "metadata": {},
   "source": [
    "## 2. Fundamento matemático\n",
    "\n",
    "### 2.1 Forward Pass (notación vectorizada)\n",
    "\n",
    "- X: (batch, n_in). Pesos: W1 (n_in, n_h), b1 (1, n_h), W2 (n_h, n_out), b2 (1, n_out).\n",
    "- Z1 = X·W1 + b1; A1 = σ(Z1).\n",
    "- Z2 = A1·W2 + b2; A2 = σ(Z2) (salida probabilística).\n",
    "\n",
    "### 2.2 Backward Pass (gradientes)\n",
    "\n",
    "- Error: E = Y - A2.\n",
    "- delta2 = E * σ'(A2)  (σ'(a)=a*(1-a) si a = σ(z)).\n",
    "- gradW2 = A1^T · delta2; gradb2 = sum(delta2, axis=0).\n",
    "- delta1 = delta2 · W2^T * σ'(A1).\n",
    "- gradW1 = X^T · delta1; gradb1 = sum(delta1, axis=0).\n",
    "- Actualizar: W += lr * gradW  (si error = Y - Yhat y usamos signo positivo en actualización como en ejemplos didácticos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7506b",
   "metadata": {},
   "source": [
    "## 3. Implementación from‑scratch\n",
    "\n",
    "A continuación definimos la clase SimpleMLP con una capa oculta y entrenamiento por batch usando backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59929a",
   "metadata": {},
   "source": [
    "## 4. Librerías, clases y funciones\n",
    "\n",
    "- numpy: operaciones vectorizadas y manejo de matrices.\n",
    "- sklearn.metrics: accuracy_score, confusion_matrix (para evaluación final).\n",
    "- Funciones definidas en código: sigmoid, sigmoid_derivative, forward, backward, train, predict_proba, predict, check_all_inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222254c8",
   "metadata": {},
   "source": [
    "## 5. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afaa90",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "Para resolver el problema XOR se seleccionó una Red Neuronal Multicapa (MLP) con una arquitectura mínima de 2–2–1 (dos neuronas de entrada, dos en la capa oculta y una de salida). Esta configuración es la más sencilla capaz de representar funciones no lineales, ya que el problema XOR no es linealmente separable, lo que impide resolverlo mediante un perceptrón simple o modelos lineales.\n",
    "\n",
    "El uso de una función de activación sigmoide en la capa oculta y en la salida permite introducir no linealidad en el modelo, lo cual es indispensable para que la red pueda aprender relaciones complejas entre las entradas y las salidas. Además, la sigmoide proporciona salidas continuas en el rango (0, 1), adecuadas para tareas de clasificación binaria como esta.\n",
    "\n",
    "La decisión de entrenar la red “from scratch” obedece a fines de transparencia y comprensión del algoritmo, permitiendo visualizar cada etapa del proceso de aprendizaje: propagación hacia adelante, cálculo del error, retropropagación de gradientes y actualización de pesos. De esta forma se obtiene una comprensión más profunda del funcionamiento interno de la retropropagación antes de usar librerías de alto nivel como scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080bf98",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4a85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación from-scratch (numpy)\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "class SimpleMLP:\n",
    "    def __init__(self, n_inputs: int, n_hidden: int, n_outputs: int, lr: float = 0.5, seed: int = 1):\n",
    "        rng = np.random.RandomState(seed)\n",
    "        # Inicialización uniforme pequeña\n",
    "        self.W1 = rng.uniform(-1.0, 1.0, (n_inputs, n_hidden))\n",
    "        self.b1 = np.zeros((1, n_hidden))\n",
    "        self.W2 = rng.uniform(-1.0, 1.0, (n_hidden, n_outputs))\n",
    "        self.b2 = np.zeros((1, n_outputs))\n",
    "        self.lr = lr\n",
    "    \n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, y: np.ndarray) -> np.ndarray:\n",
    "        # y = sigmoid(x)\n",
    "        return y * (1 - y)\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        Z1 = X.dot(self.W1) + self.b1            # (batch, n_hidden)\n",
    "        A1 = self.sigmoid(Z1)                    # (batch, n_hidden)\n",
    "        Z2 = A1.dot(self.W2) + self.b2           # (batch, n_outputs)\n",
    "        A2 = self.sigmoid(Z2)                    # (batch, n_outputs)\n",
    "        return Z1, A1, Z2, A2\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 10000, print_every: int = 1000):\n",
    "        # X: (N, n_inputs), y: (N, n_outputs)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            Z1, A1, Z2, A2 = self.forward(X)\n",
    "            error = y - A2                         # (N, n_outputs)\n",
    "            loss = np.mean(np.square(error))\n",
    "\n",
    "            # Backpropagation (vectorizado)\n",
    "            delta2 = error * self.sigmoid_deriv(A2)            # (N, n_outputs)\n",
    "            gradW2 = A1.T.dot(delta2)                          # (n_hidden, n_outputs)\n",
    "            gradb2 = np.sum(delta2, axis=0, keepdims=True)     # (1, n_outputs)\n",
    "\n",
    "            delta1 = delta2.dot(self.W2.T) * self.sigmoid_deriv(A1)  # (N, n_hidden)\n",
    "            gradW1 = X.T.dot(delta1)                                 # (n_inputs, n_hidden)\n",
    "            gradb1 = np.sum(delta1, axis=0, keepdims=True)           # (1, n_hidden)\n",
    "\n",
    "            # Actualización de pesos (nota: error = y - y_hat, por eso sumamos lr*grad)\n",
    "            self.W2 += self.lr * gradW2\n",
    "            self.b2 += self.lr * gradb2\n",
    "            self.W1 += self.lr * gradW1\n",
    "            self.b1 += self.lr * gradb1\n",
    "\n",
    "            if (epoch % print_every) == 0:\n",
    "                print(f\"Epoch {epoch}, loss={loss:.6f}\")\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        _, _, _, A2 = self.forward(X)\n",
    "        return A2\n",
    "    \n",
    "    def predict(self, X: np.ndarray, threshold: float = 0.5) -> np.ndarray:\n",
    "        probs = self.predict_proba(X)\n",
    "        return (probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8430b804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000, loss=0.001533\n",
      "Epoch 4000, loss=0.000545\n",
      "Epoch 6000, loss=0.000326\n",
      "Epoch 8000, loss=0.000231\n"
     ]
    }
   ],
   "source": [
    "# Preparar datos (tabla de verdad XOR)\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Instanciar y entrenar\n",
    "mlp = SimpleMLP(n_inputs=2, n_hidden=2, n_outputs=1, lr=0.8, seed=2)\n",
    "mlp.train(X, y, epochs=8000, print_every=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5f26d",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5a755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprobación detallada (from-scratch):\n",
      "entrada=[0 0], prob=0.0137, pred=0\n",
      "entrada=[0 1], prob=0.9855, pred=1\n",
      "entrada=[1 0], prob=0.9855, pred=1\n",
      "entrada=[1 1], prob=0.0178, pred=0\n",
      "\n",
      "Predicciones (vector): [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Predicción y comprobación por entrada\n",
    "def check_all_inputs_fromscratch(model: SimpleMLP, X: np.ndarray):\n",
    "    probs = model.predict_proba(X)\n",
    "    preds = model.predict(X)\n",
    "    for xi, pi, pr in zip(X, preds, probs):\n",
    "        print(f\"entrada={xi}, prob={pr.ravel()[0]:.4f}, pred={pi.ravel()[0]}\")\n",
    "\n",
    "print('\\nComprobación detallada (from-scratch):')\n",
    "check_all_inputs_fromscratch(mlp, X)\n",
    "\n",
    "print('\\nPredicciones (vector):', mlp.predict(X).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c4318f",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "- Las probabilidades cercanas a 0.5 pueden derivar en decisiones sensibles al umbral. Ajustar semilla/inicialización, lr o epochs puede estabilizar el mapeo exacto a la tabla XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1276f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (from-scratch): 1.0000\n",
      "Confusion Matrix:\n",
      " [[2 0]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluación: Accuracy y Confusion Matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_true = y.flatten()\n",
    "y_pred = mlp.predict(X).flatten()\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(f\"Accuracy (from-scratch): {acc:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67aa281",
   "metadata": {},
   "source": [
    "## 6. Explicación breve de parámetros y funciones clave (python)\n",
    "\n",
    "- lr (learning rate): controla el tamaño del paso en la actualización de pesos.\n",
    "- epochs: número de iteraciones completas sobre el conjunto de entrenamiento.\n",
    "- seed: semilla para inicializar aleatoriamente los pesos para reproducibilidad.\n",
    "- sigmoid / sigmoid_deriv: activación logística y su derivada, utilizadas en forward y backprop.\n",
    "- predict_proba / predict: devuelven probabilidades de clase y etiquetas binarias con umbral."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
